## üìä LLM Evaluation Libraries

| Library        | Description                                                                                                         | Link  |
|----------------|---------------------------------------------------------------------------------------------------------------------|-------|
| [Ragas](https://github.com/explodinggradients/ragas)       | Toolkit for evaluating and optimizing LLM applications.                                                | [Docs](https://docs.ragas.io/en/stable/references/integrations/) |
| [Giskard](https://github.com/Giskard-AI/giskard)          | Open-source evaluation & testing for ML & LLM systems.                                                 |       |
| [DeepEval](https://github.com/confident-ai/deepeval)      | LLM evaluation framework.                                                                              | [Docs](https://deepeval.com/docs/getting-started) |
| [Lighteval](https://github.com/huggingface/lighteval)     | All-in-one toolkit for evaluating LLMs.                                                                |       |
| [Trulens](https://github.com/truera/trulens)              | Evaluation and tracking for LLM experiments.                                                           |       |
| [PromptBench](https://github.com/microsoft/promptbench)   | A unified evaluation framework for LLMs.                                                               |       |
| [LangTest](https://github.com/JohnSnowLabs/langtest)      | 60+ test types for evaluating LLMs on accuracy, bias, fairness, robustness & more.                     |       |
| [EvalPlus](https://github.com/evalplus/evalplus)          | Rigorous evaluation framework for LLM4Code.                                                            |       |
| [FastChat](https://github.com/lm-sys/FastChat)            | Open platform for training, serving, and evaluating LLM chatbots.                                       |       |
| [judges](https://github.com/quotient-ai/judges)           | A small library of LLM judges.                                                                         |       |
| [Evals](https://github.com/openai/evals)                  | Framework for evaluating LLMs and an open-source registry of benchmarks.                               | [Docs](https://github.com/openai/evals?tab=readme-ov-file) |
| [AgentEvals](https://github.com/langchain-ai/agentevals)  | Evaluators and utilities for measuring agent performance.                                               |       |
| [LLMBox](https://github.com/RUCAIBox/LLMBox)              | Unified training pipeline and evaluation library for LLMs.                                              |       |
| [Opik](https://github.com/comet-ml/opik)                  | Open-source end-to-end LLM development & evaluation platform.                                           | [Comet](https://www.comet.com/site/pricing/) |
| [PydanticAI Evals](https://ai.pydantic.dev/evals/)        | Systematic evaluation framework for LLM applications.                                                  |       |
| [UQLM](https://github.com/cvs-health/uqlm)                | Python package for zero-resource LLM hallucination detection with uncertainty quantification.           |       |

---

## üîç More Tools to Explore

| Tool | Links |
|------|-------|
| **Helicone** | [Pricing](https://www.helicone.ai/pricing) |
| **LlamaIndex** | [Cost Analysis](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/) |
| **LangChain / LangSmith** | [Evaluation Guides](https://docs.smith.langchain.com/evaluation/how_to_guides), [Docs](https://python.langchain.com/docs/concepts/evaluation/) |
| **Humanloop** | [Docs](https://humanloop.com/docs/guides/migrating-from-humanloop) |
| **Deepchecks** | [Website](https://www.deepchecks.com/) |
| **MLflow** | [Evaluations](https://mlflow.org/genai/evaluations) |
| **LangChain OpenEvals** | [GitHub](https://github.com/langchain-ai/openevals) |
| **LangChain AgentEvals** | [GitHub](https://github.com/langchain-ai/agentevals) |



| Ragas            | Ragas is your ultimate toolkit for evaluating and optimizing Large Language Model (LLM) applications.                                   | [Link](https://github.com/explodinggradients/ragas) |     |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | --- |
| Giskard          | Open-Source Evaluation & Testing for ML & LLM systems.                                                                                  | [Link](https://github.com/Giskard-AI/giskard)       |     |
| DeepEval         | LLM Evaluation Framework                                                                                                                | [Link](https://github.com/confident-ai/deepeval)    |     |
| Lighteval        | All-in-one toolkit for evaluating LLMs.                                                                                                 | [Link](https://github.com/huggingface/lighteval)    |     |
| Trulens          | Evaluation and Tracking for LLM Experiments                                                                                             | [Link](https://github.com/truera/trulens)           |     |
| PromptBench      | A unified evaluation framework for large language models.                                                                               | [Link](https://github.com/microsoft/promptbench)    |     |
| LangTest         | Deliver Safe & Effective Language Models. 60+ Test Types for Comparing LLM & NLP Models on Accuracy, Bias, Fairness, Robustness & More. | [Link](https://github.com/JohnSnowLabs/langtest)    |     |
| EvalPlus         | A rigorous evaluation framework for LLM4Code.                                                                                           | [Link](https://github.com/evalplus/evalplus)        |     |
| FastChat         | An open platform for training, serving, and evaluating large language model-based chatbots.                                             | [Link](https://github.com/lm-sys/FastChat)          |     |
| judges           | A small library of LLM judges.                                                                                                          | [Link](https://github.com/quotient-ai/judges)       |     |
| Evals            | Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.                                    | [Link](https://github.com/openai/evals)             |     |
| AgentEvals       | Evaluators and utilities for evaluating the performance of your agents.                                                                 | [Link](https://github.com/langchain-ai/agentevals)  |     |
| LLMBox           | A comprehensive library for implementing LLMs, including a unified training pipeline and comprehensive model evaluation.                | [Link](https://github.com/RUCAIBox/LLMBox)          |     |
| Opik             | An open-source end-to-end LLM Development Platform which also includes LLM evaluation.                                                  | [Link](https://github.com/comet-ml/opik)            |     |
| PydanticAI Evals | A powerful evaluation framework designed to help you systematically evaluate the performance of LLM applications.                       | [Link](https://ai.pydantic.dev/evals/)              |     |
| UQLM | A Python package for generation-time, zero-resource LLM hallucination using state-of-the-art uncertainty quantification techniques. | [Link](https://github.com/cvs-health/uqlm) |
----------------
## References: 
https://humanloop.com/blog/best-llm-evaluation-tools
https://blog.langchain.com/evaluating-llms-with-openevals/
https://platform.openai.com/docs/guides/evals?api-mode=responses
-----
Here‚Äôs an up‚Äëto‚Äëdate breakdown of **LangSmith's pricing plans** as of early September 2025:

---

## LangSmith Pricing Plans

### Developer (Free / Entry Level)

- **Cost**: $0/month for one seat (1 user)
    
- **Traces**: Includes **5,000 base traces/month** free; additional base traces cost **$0.50 per 1,000**, and upgraded (extended) traces cost **$4.50 per 1,000** ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
    
- **Retention**: 14 days for base traces, 400 days for extended traces ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
    
- **Limits (per hour)**:
    
    - Max ingested events: 50,000 (can increase to 250,000 if payment method on file)
        
    - Total trace size stored: 500‚ÄØMB (can increase to 2.5‚ÄØGB if payment method on file) ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
        

### Plus ($39 per Seat / Month)

- **Cost**: $39/month per user, up to 10 seats ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
    
- **Traces**: Includes **10,000 base traces/month** per user; additional base traces at $0.50 per 1,000 and extended at $4.50 per 1,000 ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
    
- **Retention**: Same as Developer (14 days base, 400 days extended) ([docs.langchain.com](https://docs.langchain.com/langgraph-platform/plans?utm_source=chatgpt.com "LangGraph Platform plans - Docs by LangChain"))
    
- **Limits (per hour)**:
    
    - Max ingested events: 500,000
        
    - Trace size stored: 5‚ÄØGB ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
        
- **Extras**: 1 dev-sized LangGraph Platform deployment included; email support available ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
    

### Enterprise (Custom Pricing)

- **Cost**: Tailored to your organization's needs
    
- **Includes**: Everything from Plus, plus:
    
    - Private or hybrid deployment options (fully self-hosted or within your VPC)
        
    - Custom SSO and RBAC
        
    - Support SLA, team training, architectural guidance, deployed engineering access
        
    - Bulk data export, specialized billing, Infosec review, and more ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"), [Reddit](https://www.reddit.com/r/LangChain/comments/1bekfw8/langsmith_plus_use_in_europe_makes_you_not/?utm_source=chatgpt.com "Langsmith Plus use in Europe makes you not compliant"))
        

---

## Additional Offerings

### Startup & Education Plans

- Seed-stage startups and educational institutions can **apply for discounted or beginner-friendly pricing**, with generous free trace allowances and up to two years of support before migrating to Plus ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing")).
    

### Billing & Support

- **Developer & Plus Plans**:
    
    - Seats are billed **monthly** (prorated if added mid‚Äëmonth; no credits for removed seats)
        
    - Traces are billed **monthly in arrears** as usage accumulates ([docs.langchain.com](https://docs.langchain.com/langgraph-platform/plans?utm_source=chatgpt.com "LangGraph Platform plans - Docs by LangChain"), [langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
        
    - Support via community Slack (Developer) and prioritized email responses (Plus) ([LangSmith](https://docs.smith.langchain.com/pricing/faq?utm_source=chatgpt.com "Frequently Asked Questions | ü¶úÔ∏èüõ†Ô∏è LangSmith"))
        
- **Enterprise Plans**:
    
    - Typically billed **annually via invoice**
        
    - Includes dedicated customer success support, premium SLAs, and deployment help ([MetaCTO](https://www.metacto.com/blogs/the-true-cost-of-langsmith-a-comprehensive-pricing-integration-guide?utm_source=chatgpt.com "The True Cost of LangSmith - A Comprehensive Pricing & ..."), [LangSmith](https://docs.smith.langchain.com/pricing/faq?utm_source=chatgpt.com "Frequently Asked Questions | ü¶úÔ∏èüõ†Ô∏è LangSmith"))
        

### Data Ownership & Security

- LangSmith **does not train on your data**, and **you retain all rights to it** ([langchain.com](https://www.langchain.com/pricing?utm_source=chatgpt.com "Plans and Pricing"))
    
- On Enterprise plan, **self-hosting** ensures data never leaves your environment ([langchain.com](https://www.langchain.com/langsmith?utm_source=chatgpt.com "LangSmith"))
    
- Security compliance includes **SOC 2 Type II, GDPR, and HIPAA** (though BAA is only offered under Enterprise) ([LangSmith](https://docs.smith.langchain.com/pricing/faq?utm_source=chatgpt.com "Frequently Asked Questions | ü¶úÔ∏èüõ†Ô∏è LangSmith"))
    

---

## Quick Summary Table

|Plan|Price|Seats|Free Traces|Trace Pricing|Retention (base/extended)|Hourly Limits|Support|
|---|---|---|---|---|---|---|---|
|**Developer**|$0/month|1|5k base|$0.50 / 1k base, $4.50 / 1k extended|14d / 400d|50k events (up to 250k), 500 MB (up to 2.5 GB)|Community Slack|
|**Plus**|$39/user/month|Up to 10|10k base|$0.50 / 1k base, $4.50 / 1k extended|14d / 400d|500k events, 5 GB|Email + Slack|
|**Enterprise**|Custom pricing|Custom|Custom|Custom|Custom|Custom|Full SLA, dedicated CSM & more|
|**Startups/Edu**|Discounted|-|Generous|-|-|-|Custom onboarding|

---

Let me know if you‚Äôd like help choosing the right plan for your team size, expected trace volume, or deployment preferences!

----
Humanloop is no longer an active, standalone platform. As of now, the team behind Humanloop has been **acqui-hired** by Anthropic, and the platform itself is being **sunset**.

---

### What Happened with Humanloop?

- Humanloop announced it would **shut down the platform entirely by September 8, 2025**. After that date, its UI and APIs will no longer be available.([Weights & Biases](https://wandb.ai/wandb_fc/product-announcements-fc/reports/Humanloop-is-Sunsetting-Migrate-to-Weights-Biases-as-an-alternative--VmlldzoxMzk4ODc1Nw?utm_source=chatgpt.com "Humanloop is Sunsetting. Migrate to Weights & Biases as ..."), [Hacker News](https://news.ycombinator.com/item?id=44592216&utm_source=chatgpt.com "Tell HN: Humanloop acquired, sunsetting Sept 8th"))
    
- In parallel, **Anthropic acquired Humanloop‚Äôs team**‚Äîincluding its three co-founders (Raza Habib, Peter Hayes, and Jordan Burgess) and most of its engineers and researchers‚Äîbut **did not acquire its IP or product**.([TechCrunch](https://techcrunch.com/2025/08/13/anthropic-nabs-humanloop-team-as-competition-for-enterprise-ai-talent-heats-up/?utm_source=chatgpt.com "Anthropic nabs Humanloop team as competition for ..."))
    

---

### Is Humanloop Integrated with Anthropic?

- **Not in terms of the product or IP**. Humanloop‚Äôs platform is being retired rather than integrated into Anthropic in its original form.([TechCrunch](https://techcrunch.com/2025/08/13/anthropic-nabs-humanloop-team-as-competition-for-enterprise-ai-talent-heats-up/?utm_source=chatgpt.com "Anthropic nabs Humanloop team as competition for ..."))
    
- **Yes, in terms of talent and capabilities**. Anthropic has absorbed the Humanloop team, aiming to incorporate its expertise‚Äîparticularly in prompt management, evaluation, observability, and enterprise tooling‚Äîinto future Claude features and infrastructure.([TechCrunch](https://techcrunch.com/2025/08/13/anthropic-nabs-humanloop-team-as-competition-for-enterprise-ai-talent-heats-up/?utm_source=chatgpt.com "Anthropic nabs Humanloop team as competition for ..."))
    

---

### Summary Table

|**Aspect**|**Current Status**|
|---|---|
|Humanloop Platform|Shutting down on September 8, 2025|
|Integration|No direct product integration with Anthropic|
|Team & Talent|Acquired by Anthropic through an ‚Äúacqui-hire‚Äù|
|Future Use|Expertise likely used to enhance Claude‚Äôs enterprise tooling|

---

### What This Means for You

- If you were using Humanloop, you'll need to **export your data and migrate to an alternative platform** before the shutdown.
    
- Anthropic will **not offer the same UI or APIs**, but may develop analogous capabilities within Claude‚Äôs ecosystem, thanks to the Humanloop team‚Äôs involvement.
    
- For migration, platforms like **PromptLayer, Agenta, Keywords‚ÄØAI, Langfuse**, and others have stepped forward as viable replacements.([PromptLayer](https://blog.promptlayer.com/humanloop-shutdown-guide-to-migrating-your-prompts-and-evals-to-promptlayer/?utm_source=chatgpt.com "HumanLoop Shutdown: Guide to Migrating Your Prompts ..."), [Keywords AI](https://www.keywordsai.co/blog/humanloop-alternatives?utm_source=chatgpt.com "Humanloop Is Sunsetting ‚Äì Exploring the Best LLM‚ÄëOps ..."), [Kukarella](https://www.kukarella.com/news/anthropic-strengthens-enterprise-ai-with-humanloop-team-acquisition?utm_source=chatgpt.com "Anthropic Strengthens Enterprise AI with Humanloop Team ..."), [Future Tools](https://futuretools.beehiiv.com/p/anthropic-snaps-up-humanloop-s-humans?utm_source=chatgpt.com "Anthropic snaps up Humanloop's humans - Future Tools"))
    

---

Let me know if you'd like help evaluating migration options or exploring which alternative platform fits your needs best!