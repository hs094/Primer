
## LLM Evaluation

| Library     | Description                                                                                                         | Link  |
|------------|-----------------------------------------------------------------------------------------------------------------|-------|
| Ragas      | Ragas is your ultimate toolkit for evaluating and optimizing Large Language Model (LLM) applications.            | [Link](https://github.com/explodinggradients/ragas) |
| Giskard    | Open-Source Evaluation & Testing for ML & LLM systems.                                                           | [Link](https://github.com/Giskard-AI/giskard) |
| DeepEval | LLM Evaluation Framework | [Link](https://github.com/confident-ai/deepeval) |
| Lighteval  | All-in-one toolkit for evaluating LLMs.                                                                         | [Link](https://github.com/huggingface/lighteval) |
| Trulens | Evaluation and Tracking for LLM Experiments | [Link](https://github.com/truera/trulens) | 
| PromptBench | A unified evaluation framework for large language models.                                                        | [Link](https://github.com/microsoft/promptbench) |
| LangTest   | Deliver Safe & Effective Language Models. 60+ Test Types for Comparing LLM & NLP Models on Accuracy, Bias, Fairness, Robustness & More. | [Link](https://github.com/JohnSnowLabs/langtest) |
| EvalPlus   | A rigorous evaluation framework for LLM4Code.                                                                    | [Link](https://github.com/evalplus/evalplus) |
| FastChat   | An open platform for training, serving, and evaluating large language model-based chatbots.                      | [Link](https://github.com/lm-sys/FastChat) |
| judges     | A small library of LLM judges.                                                                                   | [Link](https://github.com/quotient-ai/judges) |
| Evals      | Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.            | [Link](https://github.com/openai/evals) |
| AgentEvals | Evaluators and utilities for evaluating the performance of your agents.                                         | [Link](https://github.com/langchain-ai/agentevals) |
| LLMBox     | A comprehensive library for implementing LLMs, including a unified training pipeline and comprehensive model evaluation. | [Link](https://github.com/RUCAIBox/LLMBox) |
| Opik       | An open-source end-to-end LLM Development Platform which also includes LLM evaluation.                           | [Link](https://github.com/comet-ml/opik) |
| PydanticAI Evals | A powerful evaluation framework designed to help you systematically evaluate the performance of LLM applications. | [Link](https://ai.pydantic.dev/evals/) |
| UQLM | A Python package for generation-time, zero-resource LLM hallucination using state-of-the-art uncertainty quantification techniques. | [Link](https://github.com/cvs-health/uqlm) |

Libraries to Explore
1. Openai/evals: https://github.com/openai/evals?tab=readme-ov-file
2. RAGAS: https://docs.ragas.io/en/stable/references/integrations/
3. Helicone: https://www.helicone.ai/pricing
4. LlamaIndex: https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/
5. Langchain/Langsmith: https://docs.smith.langchain.com/evaluation/how_to_guides, https://python.langchain.com/docs/concepts/evaluation/, 
6. Deepeval: https://deepeval.com/docs/getting-started
7. Comet: https://www.comet.com/site/pricing/
8. Humanloop: https://humanloop.com/docs/guides/migrating-from-humanloop
9. Deepchecks: https://www.deepchecks.com/
10. MLFlow: https://mlflow.org/genai/evaluations
11. Langchain OpenEvals: https://github.com/langchain-ai/openevals
12. Langchain AgentEvals: https://github.com/langchain-ai/agentevals


------
Hereâ€™s an improved version of your prompt, rewritten for clarity, consistency, and enhanced usability in Obsidian. It focuses on LLM evaluation tools, structured as a clean and readable markdown table for Obsidian, followed by a curated list of additional libraries to explore.

---

### ðŸ§ª LLM Evaluation Libraries

|**Library**|**Description**|**Link**|
|---|---|---|
|**Ragas**|Toolkit for evaluating and optimizing LLM applications.|[GitHub](https://github.com/explodinggradients/ragas)|
|**Giskard**|Open-source evaluation and testing for ML & LLM systems.|[GitHub](https://github.com/Giskard-AI/giskard)|
|**DeepEval**|LLM evaluation framework with flexible test creation and analysis tools.|[GitHub](https://github.com/confident-ai/deepeval)|
|**LightEval**|All-in-one toolkit for evaluating LLMs, backed by Hugging Face.|[GitHub](https://github.com/huggingface/lighteval)|
|**TruLens**|Evaluation and tracking framework for LLM experiments.|[GitHub](https://github.com/truera/trulens)|
|**PromptBench**|Unified evaluation framework for benchmarking large language models.|[GitHub](https://github.com/microsoft/promptbench)|
|**LangTest**|60+ test types for LLM & NLP model evaluation on accuracy, bias, fairness, robustness, and more.|[GitHub](https://github.com/JohnSnowLabs/langtest)|
|**EvalPlus**|Rigorous evaluation framework for LLM4Code.|[GitHub](https://github.com/evalplus/evalplus)|
|**FastChat**|Platform for training, serving, and evaluating LLM-based chatbots.|[GitHub](https://github.com/lm-sys/FastChat)|
|**judges**|Lightweight library of LLM judges for programmatic evaluation.|[GitHub](https://github.com/quotient-ai/judges)|
|**Evals**|OpenAI's framework for evaluating LLMs and maintaining a benchmark registry.|[GitHub](https://github.com/openai/evals)|
|**AgentEvals**|Evaluation tools for agent performance in LangChain.|[GitHub](https://github.com/langchain-ai/agentevals)|
|**LLMBox**|Library for training and evaluating LLMs with unified pipelines.|[GitHub](https://github.com/RUCAIBox/LLMBox)|
|**Opik**|End-to-end open-source LLM development platform with evaluation capabilities.|[GitHub](https://github.com/comet-ml/opik)|
|**PydanticAI Evals**|Framework to systematically evaluate LLM applications using Pydantic validation.|[Website](https://ai.pydantic.dev/evals/)|
|**UQLM**|Zero-resource hallucination detection via uncertainty quantification.|[GitHub](https://github.com/cvs-health/uqlm)|

---

### ðŸ“š Additional Libraries to Explore

- [OpenAI Evals](https://github.com/openai/evals?tab=readme-ov-file)
    
- [RAGAS Docs](https://docs.ragas.io/en/stable/references/integrations/)
    
- [Helicone](https://www.helicone.ai/pricing)
    
- [LlamaIndex Evaluation](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/)
    
- [LangChain / LangSmith Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides)
    
- [LangChain Evaluation Concepts](https://python.langchain.com/docs/concepts/evaluation/)
    
- [DeepEval Docs](https://deepeval.com/docs/getting-started)
    
- [Comet ML](https://www.comet.com/site/pricing/)
    
- [Humanloop](https://humanloop.com/docs/guides/migrating-from-humanloop)
    
- [Deepchecks](https://www.deepchecks.com/)
    
- [MLflow GenAI Evaluations](https://mlflow.org/genai/evaluations)
    
- [LangChain OpenEvals](https://github.com/langchain-ai/openevals)
    
- [LangChain AgentEvals](https://github.com/langchain-ai/agentevals)
    

---

### âœ… Notes for Obsidian

- Use the Markdown view to render tables properly.
    
- Internal link tagging (e.g., `[[ToolName]]`) can be added manually if using internal notes.
    
- Group similar tools (e.g., agent-based, code-focused, bias-testing) for deeper analysis if needed.
    

---

Would you like to turn this into a template that auto-updates or adds metadata for each tool?

-------
